---
title: "Applied Statistical Programming - Spring 2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\begin{center}
{\Large{\textbf{Problem Set 3}}} \\
\vspace{4 bp}
Due Wednesday, March 16, 10:00 AM (Before Class) \\
\end{center}

\section*{Instructions}
\begin{enumerate}
  \item The following questions should each be answered within an Rmarkdown file. Be sure to provide many comments in your code blocks to facilitate grading. Undocumented code will not be graded.
  \item Work on git. Continue to work in the repository you forked from \url{https://github.com/johnsontr/AppliedStatisticalProgramming2022} and add your code for Problem Set 4. Commit and push frequently. Use meaningful commit messages because these will affect your grade.
  \item You may work in teams, but each student should develop their own Rmarkdown file. To be clear, there should be no copy and paste. Each keystroke in the assignment should be your own.
  \item For students new to programming, this may take a while. Get started.
\end{enumerate}

\section*{\texttt{tidyverse}}

Your task in this problem set is to combine two datasets in order to observe how many endorsements each candidate received using only \texttt{dplyr} functions. Use the same Presidential primary polls that were used for the in class worksheets on February 28 and March 2.

```{r tidy = TRUE}
library(fivethirtyeight)
library(tidyverse)
# URL to the data that you've used.
url <- 'https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv'
polls <- read_csv(url)
Endorsements <- endorsements_2020 # from the fiverthirtyeight package
```

First, create two new objects \texttt{polls} and \texttt{Endorsements}.

Then complete the following.
\begin{itemize}
  \item Change the \texttt{Endorsements} variable name endorsee to \texttt{candidate\_name}.
  \item Change the \texttt{Endorsements} dataframe into a \texttt{tibble} object.
  \item Filter the \texttt{poll} variable to only include the following 6 candidates: Amy Klobuchar, Bernard Sanders,Elizabeth Warren, Joseph R. Biden Jr., Michael Bloomberg, Pete Buttigieg \textbf{and} subset the dataset to the following five variables: \texttt{candidate\_name, sample\_size, start\_date, party, pct}
  \item Compare the candidate names in the two datasets and find instances where the a candidates name is spelled differently i.e. Bernard vs. Bernie. Using only \texttt{dplyr} functions, make these the same across datasets. 
  \item Now combine the two datasets by candidate name using \texttt{dplyr} (there will only be five candidates after joining).
  \item Compare the candidate names in the two datasets and find instances where the a candidates name is spelled differently i.e. Bernard vs. Bernie. Using only \texttt{dplyr} functions, make these the same across datasets. 
  \item Create a variable which indicates the number of endorsements for each of the five candidates using \texttt{dplyr}.
  \item Plot the number of endorsement each of the 5 candidates have using \texttt{ggplot()}. Save your plot as an object \texttt{p}.
  \item Rerun the previous line as follows: \texttt{p + theme\_dark()}. Notice how you can still customize your plot without rerunning the plot with new options.
  \item Now, using the knowledge from the last step change the label of the X and Y axes to be more informative, add a title. Save the plot in your forked repository.
\end{itemize}

```{r tidy = TRUE}
Endorsements <- Endorsements %>%
  rename(candidate_name = endorsee)
# FINISHED!

Endorsements <- as_tibble(Endorsements)
class(Endorsements)
# FINISHED!

polls <- polls %>%
  filter(candidate_name %in% c("Amy Klobuchar",
                               "Bernard Sanders",
                               "Elizabeth Warren",
                               "Joseph R. Biden Jr.",
                               "Michael Bloomberg",
                               "Pete Buttigieg")) %>%
  select(candidate_name, sample_size, start_date, party, pct)
# FINISHED!

# Compare candidate names.
eucn <- summarize(Endorsements, endorsements_unique_candidate_names = unique(candidate_name))
#eucn

pucn <- summarize(polls, polls_unique_candidate_names = unique(candidate_name))
#pucn
# Same names (FORMAT - candidate_name  candidate_name):
## Bernard Sanders      Bernie Sanders
## Pete Buttigieg			  Pete Buttigieg (same)
## Joseph R. Biden Jr.  Joe Biden
## Amy Klobuchar			  Amy Klobuchar (same)
## Elizabeth Warren		  Elizabeth Warren (same)
## Michael Bloomberg    NONE (not in Endorsements dataset)
rm(eucn, pucn)
# Change candidate names so they are equivalent.
polls <- polls %>%
  mutate(candidate_name = replace(candidate_name,
                                  candidate_name == "Bernard Sanders",
                                  "Bernie Sanders")
         ) %>%
  mutate(candidate_name = replace(candidate_name,
                                  candidate_name == "Joseph R. Biden Jr.",
                                  "Joe Biden")
         )
# FINISHED!

# No shared variables besides candidate_name.
# merge (use inner_join to keep only those ones that match)
combined <- inner_join(polls, Endorsements, by = "candidate_name")
dim(combined)
summarize(combined, endorsements_unique_candidate_names = unique(candidate_name))
# Only 5 unique candidates left.
# FINISHED!

NumberEndorsements <- Endorsements %>%
  group_by(candidate_name) %>%
  mutate(NumberEndorsements = n()) %>%
  select(candidate_name, NumberEndorsements) %>%
  unique() %>%
  filter(candidate_name %in% c("Amy Klobuchar",
                               "Bernie Sanders",
                               "Elizabeth Warren",
                               "Joe Biden",
                               "Pete Buttigieg")
         )
NumberEndorsements
# FINISHED!

library(ggplot2)
p <- ggplot(data = NumberEndorsements,
       mapping = aes(x = candidate_name, y = NumberEndorsements)) +
  geom_bar(stat = "identity",
           fill="steelblue")
p
# FINISHED!

p + theme_dark()
# FINISHED! 

p <- p +
  geom_text(aes(label = NumberEndorsements),
            vjust = -0.3, 
            size = 3.5) +
  labs(title = "2020 Democratic Presidential Primary Endorsements",
       subtitle = "Number of Endorsements By Candidate") +
  ylab("Number of Endorsements") + 
  xlab("2020 Democratic Presidential Primary Candidates") +
  theme_minimal()
p
ggsave("PS4_p_plot.png")

# FINISHED!

ls()
rm(combined, Endorsements, NumberEndorsements, p, polls, url)
```




\section*{Text-as-Data with \texttt{tidyverse}}

For this question you will be analyzing Tweets from President Trump for various characteristics. Load in the following packages and data:

```{r}
# Change eval=FALSE in the code block. Install packages as appropriate.
library(tidyverse)
# install.packages('tm')
library(tm) 
#install.packages('lubridate')
library(lubridate)
# install.packages('wordcloud')
library(wordcloud)
trump_tweets_url <- 'https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv'
tweets <- read_csv(trump_tweets_url)
```



First separate the \texttt{created\_at} variable into two new variables where the date and the time are in separate columns. After you do that, then report the range of dates that is in this dataset.

```{r tidy = TRUE}
# Investigate Data:
colnames(tweets)
tail(tweets$created_at)
class(tweets$created_at)

# Separate 'created_at' variable
tweets <- tweets %>%
  separate(created_at,
           c("date", "time"),
           " ")

# Report the range of dates in dataset:
tweets <- tweets %>%
  mutate(date = as.Date(date, 
                        "%m/%d/%Y"))
class(tweets$date)
summary(tweets$date)

# Earliest tweet is January 1st, 2014.
# Latest tweet is February 14, 2020.

# FINISHED!
```

Using \texttt{dplyr} subset the data to only include original tweets (remove retweets) and show the text of the President's \textbf{top 5} most popular and most retweeted tweets. (Hint: The \texttt{match} function can help you find the index once you identify the largest values.) 

```{r tidy = TRUE}
# Create subset of 'tweets' dataset that includes only original tweets.
subtweets <- tweets %>%
  filter(is_retweet == FALSE)

class(subtweets$favorite_count)
class(subtweets$retweet_count)
colnames(subtweets)

# Find top 5 favorited tweets:
top5fav <- subtweets %>%
  arrange(desc(favorite_count)) %>%
  slice(1:5)
top5fav

# Find text of top 5 favorited tweets:
top5fav_text <- top5fav %>%
  select(text)

# Find top 5 retweeted tweets:
top5retweets <- subtweets %>%
  arrange(desc(retweet_count)) %>%
  slice(1:5)

# Find text of top 5 retweeted tweets:
top5retweets_text <- top5retweets %>%
  select(text)

# Create one item out of both:
t5favretweets_text <- rbind(top5fav_text, top5retweets_text)

# Turn into character vector:
t5favretweets_text <- t5favretweets_text %>%
  pull(text)
t5favretweets_text

class(t5favretweets_text)
```

Create a \textit{corpus} of the tweet content and put this into the object \texttt{Corpus} using the \texttt{tm} (text mining) package. (Hint: Do the assigned readings.)

```{r tidy = TRUE}
#vignette("tm")

# Create Corpus object:
Corpus <- VCorpus(VectorSource(t5favretweets_text))
inspect(Corpus[[1]])
```

Remove extraneous whitespace, remove numbers and punctuation, convert everything to lower case and remove 'stop words' that have little substantive meaning (the, a, it).

```{r tidy = TRUE}
# Remove extraneous whitespace:
Corpus <- tm_map(Corpus, stripWhitespace)

# Remove numbers:
Corpus <- tm_map(Corpus, 
                 content_transformer(removeNumbers)
                 )

# Remove punctuation:
Corpus <- tm_map(Corpus, 
                 content_transformer(removePunctuation)
                 )

# Convert everything to lower case:
Corpus <- tm_map(Corpus, 
                 content_transformer(tolower)
                 )

# Remove 'stop words' with little substantive meaning:
Corpus <- tm_map(Corpus,
                 removeWords,
                 stopwords("english")
                 )

inspect(Corpus[[2]])
```

Now create a \texttt{wordcloud} to visualize the top 50 words the President uses in his tweets. Use only words that occur at least three times. Display the plot with words in random order and use 50 random colors. Save the plot into your forked repository.

```{r tidy = TRUE}
# To create a word matrix, I need to create a document term matrix (as is requested in the next section).
doctermat <- TermDocumentMatrix(Corpus)
doctermat <- as.matrix(doctermat)
doctermat <- sort(rowSums(doctermat),
                  decreasing = TRUE)
doctermat <- data.frame(word = names(doctermat),
                        freq = doctermat)
doctermat <- doctermat %>%
  filter(freq >= 3)
doctermat

# Huh? There are only 6 words with a frequency greater than three. I think we need more.
### I'm going to use the top 100 most-favorited tweets instead.

# Redoing all the above
subtweets <- tweets %>%
  filter(is_retweet == FALSE)
# Find top 100 favorited tweets:
top100fav <- subtweets %>%
  arrange(desc(favorite_count)) %>%
  slice(1:100)
# Find text of top 100 favorited tweets:
top100fav_text <- top100fav %>%
  select(text)
# Turn into character vector:
top100fav_text <- top100fav_text %>%
  pull(text)
# Create Corpus object:
Corpus <- VCorpus(VectorSource(top100fav_text))
# Remove extraneous whitespace:
Corpus <- tm_map(Corpus, stripWhitespace)
# Remove numbers:
Corpus <- tm_map(Corpus, 
                 content_transformer(removeNumbers)
                 )
# Remove punctuation:
Corpus <- tm_map(Corpus, 
                 content_transformer(removePunctuation)
                 )
# Convert everything to lower case:
Corpus <- tm_map(Corpus, 
                 content_transformer(tolower)
                 )
# Remove 'stop words' with little substantive meaning:
Corpus <- tm_map(Corpus,
                 removeWords,
                 stopwords("english")
                 )
# Create a document term matrix:
doctermat <- TermDocumentMatrix(Corpus)
doctermat <- as.matrix(doctermat)
doctermat <- sort(rowSums(doctermat),
                  decreasing = TRUE)
doctermat <- data.frame(word = names(doctermat),
                        freq = doctermat)
doctermat <- doctermat %>%
  filter(freq >= 3)

# Some last fixes!
doctermat <- doctermat %>%
  filter(word != "amp") %>%
  mutate(word = replace(word, word == "aap", "a$ap"))
doctermat <- head(doctermat, 50)
doctermat

## add in color:
#library(RColorBrewer)
color_options <- colors() %>%
  sample(size = 50, 
         replace = FALSE
         )
#colors

# Save:
png("./probset4_wordcloud.png")

# Create wordcloud:
#?wordcloud()
wordcloud(doctermat$word,
          doctermat$freq,
          random.color = TRUE,
          colors = color_options,
          random.order = FALSE)
```

Create a \textit{document term matrix} called \texttt{DTM} that includes the argument \texttt{ control = list(weighting = weightTfIdf)}

```{r tidy = TRUE}
doctermat <- TermDocumentMatrix(Corpus, control = list(weighting = weightTfIdf))
doctermat
```

Finally, report the 50 words with the the highest tf.idf scores using a lower frequency bound of .8.

```{r tidy = TRUE}
# This is largely a repeat of the section before the wordcloud:
doctermat2 <- as.matrix(doctermat)
doctermat2 <- sort(rowSums(doctermat2),
                  decreasing = TRUE)
doctermat2 <- data.frame(word = names(doctermat2),
                        weightTfIdf = doctermat2)
doctermat2 <- doctermat2 %>%
  filter(weightTfIdf >= 0.8)
doctermat2 <- head(doctermat2, 50)
doctermat2
```

